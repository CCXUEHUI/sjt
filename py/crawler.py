import os
import time
import shutil
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service

BASE_URL = "https://m.tuiimg.com/meinv/"
IMG_DIR = "images"
TXT_PATH = os.path.join(IMG_DIR, "files.txt")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Mobile Safari/537.36 EdgA/136.0.0.0"
}

# åˆ›å»º images æ–‡ä»¶å¤¹
os.makedirs(IMG_DIR, exist_ok=True)

# å·²ä¿å­˜çš„åœ°å€é›†åˆ
existing_urls = set()
if os.path.exists(TXT_PATH):
    with open(TXT_PATH, "r", encoding="utf-8") as f:
        existing_urls = set(line.strip() for line in f if line.strip())

def is_landscape(img: Image.Image) -> bool:
    return img.width > img.height

def save_image(url: str):
    if url in existing_urls:
        print(f"ğŸ” å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{url}")
        return
    try:
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½å›¾ç‰‡ï¼š{url}")
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        img = Image.open(BytesIO(resp.content))
        print(f"ğŸ“ å›¾ç‰‡å°ºå¯¸ï¼š{img.width}x{img.height}")
        if is_landscape(img):
            filename = os.path.basename(url)
            path = os.path.join(IMG_DIR, filename)
            img.save(path)
            with open(TXT_PATH, "a", encoding="utf-8") as f:
                f.write(url + "\n")
            print(f"âœ… å·²ä¿å­˜æ¨ªå›¾ï¼š{filename}")
        else:
            print(f"â›” è·³è¿‡ç«–å›¾ï¼š{url}")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url}ï¼Œé”™è¯¯ï¼š{e}")

def get_subpages():
    print(f"ğŸŒ æ­£åœ¨è®¿é—®ä¸»é¡µé¢ï¼š{BASE_URL}")
    resp = requests.get(BASE_URL, headers=HEADERS, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    subpages = set()
    for li in soup.find_all("li"):
        for a in li.find_all("a", href=True):
            href = a["href"]
            if href.startswith("https://m.tuiimg.com/meinv/"):
                subpages.add(href)

    print(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(subpages)} ä¸ªæœ‰æ•ˆå­é¡µé¢é“¾æ¥")
    return list(subpages)

def get_chrome_path():
    candidates = [
        shutil.which("google-chrome-stable"),
        shutil.which("google-chrome"),
        shutil.which("chromium-browser"),
        "/opt/google/chrome/google-chrome"
    ]
    for path in candidates:
        if path and os.path.exists(path) and os.access(path, os.X_OK):
            print(f"âœ… ä½¿ç”¨ Chrome è·¯å¾„: {path}")
            return path
    raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„ Chrome å¯æ‰§è¡Œæ–‡ä»¶")

def get_chromedriver_path():
    driver_path = shutil.which("chromedriver")
    if driver_path and os.path.exists(driver_path) and os.access(driver_path, os.X_OK):
        print(f"âœ… ä½¿ç”¨ chromedriver è·¯å¾„: {driver_path}")
        return driver_path
    raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„ chromedriver")

def extract_image_urls(page_url):
    print(f"ğŸ“„ æ‰“å¼€å­é¡µé¢ï¼š{page_url}")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")

    chrome_path = get_chrome_path()
    options.binary_location = chrome_path

    driver_path = get_chromedriver_path()
    driver = webdriver.Chrome(service=Service(driver_path), options=options)

    driver.get(page_url)
    time.sleep(3)

    # æ¨¡æ‹Ÿç‚¹å‡»â€œå±•å¼€å…¨å›¾â€
    try:
        expand_btn = driver.find_element(By.XPATH, "//a[contains(text(),'å±•å¼€å…¨å›¾')]")
        expand_btn.click()
        time.sleep(3)
    except Exception:
        print("âš ï¸ æœªæ‰¾åˆ°å±•å¼€æŒ‰é’®ï¼Œå¯èƒ½é¡µé¢å·²ç›´æ¥æ˜¾ç¤ºå…¨éƒ¨å›¾ç‰‡")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    img_urls = set()
    for img in soup.find_all("img", src=True):
        src = img["src"]
        if src.startswith("https://i.tuiimg.net") and src.endswith(".jpg"):
            img_urls.add(src)

    print(f"ğŸ–¼ï¸ æå–åˆ° {len(img_urls)} å¼ å›¾ç‰‡")
    return list(img_urls)

def clean_files_txt():
    if os.path.exists(TXT_PATH):
        with open(TXT_PATH, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
        with open(TXT_PATH, "w", encoding="utf-8") as f:
            f.write("\n".join(sorted(set(lines))) + "\n")
        print(f"ğŸ§¹ å·²æ¸…ç† files.txtï¼Œå½“å‰è®°å½• {len(lines)} æ¡")

def main():
    subpages = get_subpages()
    if not subpages:
        print("âš ï¸ æ²¡æœ‰å­é¡µé¢ï¼Œç»ˆæ­¢ä»»åŠ¡")
        return
    for page in subpages:
        img_urls = extract_image_urls(page)
        for url in img_urls:
            save_image(url)
    clean_files_txt()

if __name__ == "__main__":
    main()
