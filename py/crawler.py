import os
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

BASE_URL = "https://m.tuiimg.com/meinv/"
IMG_DIR = "images"
TXT_PATH = os.path.join(IMG_DIR, "files.txt")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Mobile Safari/537.36 EdgA/136.0.0.0"
}

# åˆ›å»º images æ–‡ä»¶å¤¹
os.makedirs(IMG_DIR, exist_ok=True)

# å·²ä¿å­˜çš„åœ°å€é›†åˆ
existing_urls = set()
if os.path.exists(TXT_PATH):
    with open(TXT_PATH, "r", encoding="utf-8") as f:
        existing_urls = set(line.strip() for line in f if line.strip())

def is_landscape(img: Image.Image) -> bool:
    return img.width > img.height

def save_image(url: str):
    if url in existing_urls:
        print(f"ğŸ” å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{url}")
        return
    try:
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½å›¾ç‰‡ï¼š{url}")
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        img = Image.open(BytesIO(resp.content))
        print(f"ğŸ“ å›¾ç‰‡å°ºå¯¸ï¼š{img.width}x{img.height}")
        if is_landscape(img):
            filename = os.path.basename(url)
            path = os.path.join(IMG_DIR, filename)
            img.save(path)
            with open(TXT_PATH, "a", encoding="utf-8") as f:
                f.write(url + "\n")
            print(f"âœ… å·²ä¿å­˜æ¨ªå›¾ï¼š{filename}")
        else:
            print(f"â›” è·³è¿‡ç«–å›¾ï¼š{url}")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url}ï¼Œé”™è¯¯ï¼š{e}")

def get_subpages():
    try:
        print(f"ğŸŒ æ­£åœ¨è®¿é—®ä¸»é¡µé¢ï¼š{BASE_URL}")
        resp = requests.get(BASE_URL, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # åœ¨ li æ ‡ç­¾ä¸‹æŸ¥æ‰¾æ‰€æœ‰ a æ ‡ç­¾
        links = soup.find_all("li")
        subpages = set()
        for li in links:
            a_tags = li.find_all("a", href=True)
            for a in a_tags:
                href = a["href"]
                if href.startswith("https://m.tuiimg.com/meinv/"):
                    subpages.add(href)

        print(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(subpages)} ä¸ªæœ‰æ•ˆå­é¡µé¢é“¾æ¥")
        return list(subpages)
    except Exception as e:
        print(f"âŒ è·å–å­é¡µé¢å¤±è´¥ï¼š{e}")
        return []

def extract_image_urls(page_url):
    try:
        print(f"ğŸ“„ è®¿é—®å­é¡µé¢ï¼š{page_url}")
        resp = requests.get(page_url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # æ¨¡æ‹Ÿç‚¹å‡»â€œå±•å¼€å…¨å›¾â€ï¼Œç›´æ¥æŸ¥æ‰¾å®Œæ•´é¡µé¢ä¸­çš„å›¾ç‰‡
        img_tags = soup.find_all("img", src=True)
        img_urls = set()
        for img in img_tags:
            src = img["src"]
            if src.startswith("https://i.tuiimg.net") and src.endswith(".jpg"):
                img_urls.add(src)

        print(f"ğŸ–¼ï¸ æå–åˆ° {len(img_urls)} å¼ å›¾ç‰‡")
        return list(img_urls)
    except Exception as e:
        print(f"âŒ æå–å›¾ç‰‡å¤±è´¥ï¼š{page_url}ï¼Œé”™è¯¯ï¼š{e}")
        return []

def main():
    subpages = get_subpages()
    if not subpages:
        print("âš ï¸ æ²¡æœ‰å­é¡µé¢ï¼Œç»ˆæ­¢ä»»åŠ¡")
        return
    for page in subpages:
        img_urls = extract_image_urls(page)
        for url in img_urls:
            save_image(url)

if __name__ == "__main__":
    main()
