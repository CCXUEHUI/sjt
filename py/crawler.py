import os
import time
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

BASE_URL = "https://m.tuiimg.com/meinv/"
IMG_DIR = "images"
TXT_PATH = os.path.join(IMG_DIR, "files.txt")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Mobile Safari/537.36 EdgA/136.0.0.0"
}

# åˆ›å»º images æ–‡ä»¶å¤¹
os.makedirs(IMG_DIR, exist_ok=True)

# å·²ä¿å­˜çš„åœ°å€é›†åˆ
existing_urls = set()
if os.path.exists(TXT_PATH):
    with open(TXT_PATH, "r", encoding="utf-8") as f:
        existing_urls = set(line.strip() for line in f if line.strip())

def is_landscape(img: Image.Image) -> bool:
    return img.width > img.height

def save_image(url: str):
    if url in existing_urls:
        print(f"ğŸ” å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{url}")
        return
    try:
        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½å›¾ç‰‡ï¼š{url}")
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        img = Image.open(BytesIO(resp.content))
        print(f"ğŸ“ å›¾ç‰‡å°ºå¯¸ï¼š{img.width}x{img.height}")
        if is_landscape(img):
            filename = os.path.basename(url)
            path = os.path.join(IMG_DIR, filename)
            img.save(path)
            with open(TXT_PATH, "a", encoding="utf-8") as f:
                f.write(url + "\n")
            print(f"âœ… å·²ä¿å­˜æ¨ªå›¾ï¼š{filename}")
        else:
            print(f"â›” è·³è¿‡ç«–å›¾ï¼š{url}")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url}ï¼Œé”™è¯¯ï¼š{e}")

def get_subpages():
    print(f"ğŸŒ æ­£åœ¨è®¿é—®ä¸»é¡µé¢ï¼š{BASE_URL}")
    resp = requests.get(BASE_URL, headers=HEADERS, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    subpages = set()
    for li in soup.find_all("li"):
        for a in li.find_all("a", href=True):
            href = a["href"]
            if href.startswith("https://m.tuiimg.com/meinv/"):
                subpages.add(href)

    print(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(subpages)} ä¸ªæœ‰æ•ˆå­é¡µé¢é“¾æ¥")
    return list(subpages)

def extract_image_urls(page_url):
    print(f"ğŸ“„ æ‰“å¼€å­é¡µé¢ï¼š{page_url}")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.binary_location = "/usr/bin/google-chrome"  # æŒ‡å®š Chrome è·¯å¾„
    driver = webdriver.Chrome(options=options)

    driver.get(page_url)
    time.sleep(3)

    # æ¨¡æ‹Ÿç‚¹å‡»â€œå±•å¼€å…¨å›¾â€
    try:
        expand_btn = driver.find_element(By.XPATH, "//a[contains(text(),'å±•å¼€å…¨å›¾')]")
        expand_btn.click()
        time.sleep(3)
    except Exception:
        print("âš ï¸ æœªæ‰¾åˆ°å±•å¼€æŒ‰é’®ï¼Œå¯èƒ½é¡µé¢å·²ç›´æ¥æ˜¾ç¤ºå…¨éƒ¨å›¾ç‰‡")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    img_urls = set()
    for img in soup.find_all("img", src=True):
        src = img["src"]
        if src.startswith("https://i.tuiimg.net") and src.endswith(".jpg"):
            img_urls.add(src)

    print(f"ğŸ–¼ï¸ æå–åˆ° {len(img_urls)} å¼ å›¾ç‰‡")
    return list(img_urls)

def clean_files_txt():
    if os.path.exists(TXT_PATH):
        with open(TXT_PATH, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
        with open(TXT_PATH, "w", encoding="utf-8") as f:
            f.write("\n".join(sorted(set(lines))) + "\n")
        print(f"ğŸ§¹ å·²æ¸…ç† files.txtï¼Œå½“å‰è®°å½• {len(lines)} æ¡")

def main():
    subpages = get_subpages()
    if not subpages:
        print("âš ï¸ æ²¡æœ‰å­é¡µé¢ï¼Œç»ˆæ­¢ä»»åŠ¡")
        return
    for page in subpages:
        img_urls = extract_image_urls(page)
        for url in img_urls:
            save_image(url)
    clean_files_txt()

if __name__ == "__main__":
    main()
